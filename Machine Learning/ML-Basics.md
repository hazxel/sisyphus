# 激活函数

激活函数能使得神经网络的每层输出结果变得非线性化（目标问题不一定是线性的，神经网络需要拟合任意函数）

### Sigmoid - $f(x)=\frac1{1+e^{-x}}$

sigmod是最早被应用的激活函数。它在原点附近的梯度近似线性，而在远端的梯度近似为零，可解决梯度爆炸问题，但无法很好解决梯度消失问题。

### tanh

### logistic

### ReLU - $f(x)=\max(0,x)$

存在 Dead ReLU 问题。当输入为负时，ReLU 完全失效，在反向传播过程中梯度将完全为零无法更新

### leaky ReLU - $f(x)=\max(0,x) + \gamma\min(0,x)$

取很小的 $\gamma$，使负数输入仍然存在很小的梯度，以解决 Dead ReLU 问题。

### ELU (exponential linear unit)

同样也是针对解决 ReLU负数部分存在的问题，（负数部分是个很平的 exponential 函数）。ELU的缺点是计算度慢，但在训练过程中，更快的收敛速度补偿了这一点。

### 总结

选择激活函数时，一般可以按照这个顺序：ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic。



# 反向传播

成本函数，或代价函数，损失函数，被用来衡量模型的错误程度。

为了优化模型以获得更小的误差，我们希望使用梯度下降算法，计算损失函数的梯度并沿梯度的相反方向移动，指导搜索局部最小值。成本函数的偏导数影响最终模型的权重和选择的偏差。

当目标函数不复杂时，常使用差分法，即在某一个维度施加一个微小误差后观察损失函数的变化，获取近似梯度。但在深度学习场景下，目标函数很复杂，其本身的计算已经相当耗时，再加上参数的个数很多，采用差分法为每个参数求解偏导数都要计算两次损失函数，效率极其低下。

因此，反向传播应运而生。它从后向前，使用后一层的梯度信息来计算前一层的梯度，即通过计算前一层中各神经元为该输出结果贡献的误差，反复向前迭代，直到算法到达初始的输入层为止，避免了多次计算目标函数。



# 优化算法

### 问题定义

连续优化问题的规范形是: （以最小化为例）
$$
\begin{aligned}
\min_x &f(x) \\
subject \,\, to \,\, &g_i(x) \leq0, i = 1,...,m \\
&h_j(x) = 0, j = 1, ..., p
\end{aligned}
$$

### 梯度下降 SGD

SGD 是一个一阶最优化算法，可用于求解最优化问题，是神经网络模型训练最常用的优化算法。对目标函数 $f(\pmb x)$，迭代公式为： $x_i = x_{i-1} - \eta \cdot \nabla f(x_{i-1}) $

> vs 最小二乘
>
> 梯度下降法是x迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

### 动量梯度下降法 Momentum SGD

在动量梯度下降算法中，每次迭代的 $\Delta$ 不再单纯的为梯度值，而是变为一个冲量项 $m$，其中的超参数 $\gamma$ 一般取接近 1 的值如 0.9，这意味着 $m$ 等价于梯度的指数加权移动平均值，所以可以减少训练的震荡过程。
$$
m_i = \gamma \cdot m_{i-1} + \eta \cdot\nabla f(x_{i-1})\\
x_{i} = x_{i-1} - m
$$

### AdaGrad ？？？

AdaGrad是一种学习速率自适应的梯度下降算法。在训练迭代过程中其学习速率逐渐衰减，经常更新的参数其学习速率衰减更快。

### RMSProp ？？？

其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。思路类似Momentum SGD，引入一个超参数，在积累梯度平方项进行衰减。

### Adaptive moment estimation (Adam) ？？？

Adam结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。







## 效能衡量指標

##### 混淆矩阵

1. tp：事实为真，预测为真(p)，预测正确(t)
2. tn：事实为假，预测为假(n)，预测正确(t)
3. fp：事实为假，预测为真(p)，预测错误(f)
4. fn：事实为真，预测为假(n)，预测错误(f)



準確率（Accuracy）= (tp+tn)/(tp+fp+fn+tn)
精確率（Precision）= tp/(tp+fp)，即陽性的樣本中有幾個是預測正確的。
召回率（Recall）= tp/(tp+fn)，即事實為真的樣本中有幾個是預測正確的。
F1 score = 2 / ( (1/ Precision) + (1/ Recall) )，即精確率與召回率的調和平均數。

一般情況下，我們只要使用準確率(Accuracy)衡量模型的效能即可，就是『預測正確的比率』，即猜對的個數(tp+tn) / 全部樣本數。但是，當訓練資料的目標變數不平衡(imbalance)時就要小心了。

舉一個例子，根據統計美國每年有1000萬人次出入境，假設有10個恐怖份子試圖闖關，海關有抓到5個嫌疑犯，如果以準確率計算，(1000萬 - (10-5))/1000萬=99.9999..%，準確率高得嚇人，但是美國政府會滿意嗎? 事實上有一半的恐怖份子沒抓到，這時如果改用召回率（Recall）計算，5/10=50%，就合理多了。