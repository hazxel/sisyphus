# 激活函数

激活函数能使得神经网络的每层输出结果变得非线性化（目标问题不一定是线性的，神经网络需要拟合任意函数）

### Sigmoid - $f(x)=\frac1{1+e^{-x}}$

sigmod是最早被应用的激活函数。它在原点附近的梯度近似线性，而在远端的梯度近似为零，可解决梯度爆炸问题，但无法很好解决梯度消失问题。

### tanh

### logistic

### ReLU - $f(x)=\max(0,x)$

存在 Dead ReLU 问题。当输入为负时，ReLU 完全失效，在反向传播过程中梯度将完全为零无法更新

### leaky ReLU - $f(x)=\max(0,x) + \gamma\min(0,x)$

取很小的 $\gamma$，使负数输入仍然存在很小的梯度，以解决 Dead ReLU 问题。

### ELU (exponential linear unit)

同样也是针对解决 ReLU负数部分存在的问题，（负数部分是个很平的 exponential 函数）。ELU的缺点是计算度慢，但在训练过程中，更快的收敛速度补偿了这一点。

### 总结

选择激活函数时，一般可以按照这个顺序：ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic。



# 反向传播

成本函数，或代价函数，损失函数，被用来衡量模型的错误程度。

为了优化模型以获得更小的误差，我们希望使用梯度下降算法，计算损失函数的梯度并沿梯度的相反方向移动，指导搜索局部最小值。成本函数的偏导数影响最终模型的权重和选择的偏差。

当目标函数不复杂时，常使用差分法，即在某一个维度施加一个微小误差后观察损失函数的变化，获取近似梯度。但在深度学习场景下，目标函数很复杂，其本身的计算已经相当耗时，再加上参数的个数很多，采用差分法为每个参数求解偏导数都要计算两次损失函数，效率极其低下。

因此，反向传播应运而生。它从后向前，使用后一层的梯度信息来计算前一层的梯度，即通过计算前一层中各神经元为该输出结果贡献的误差，反复向前迭代，直到算法到达初始的输入层为止，避免了多次计算目标函数。



# 优化算法

### 问题定义

连续优化问题的规范形是: （以最小化为例）
$$
\begin{aligned}
\min_x &f(x) \\
subject \,\, to \,\, &g_i(x) \leq0, i = 1,...,m \\
&h_j(x) = 0, j = 1, ..., p
\end{aligned}
$$

### 梯度下降 SGD

SGD 是一个一阶最优化算法，可用于求解最优化问题，是神经网络模型训练最常用的优化算法。对目标函数 $f(\pmb x)$，迭代公式为： $x_i = x_{i-1} - \eta \cdot \nabla f(x_{i-1}) $

> vs 最小二乘
>
> 梯度下降法是x迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。

### 动量梯度下降法 Momentum SGD

在动量梯度下降算法中，每次迭代的 $\Delta$ 不再单纯的为梯度值，而是变为一个冲量项 $m$，其中的超参数 $\gamma$ 一般取接近 1 的值如 0.9，这意味着 $m$ 等价于梯度的指数加权移动平均值，所以可以减少训练的震荡过程。
$$
m_i = \gamma \cdot m_{i-1} + \eta \cdot\nabla f(x_{i-1})\\
x_{i} = x_{i-1} - m
$$

### AdaGrad ？？？

AdaGrad是一种学习速率自适应的梯度下降算法。在训练迭代过程中其学习速率逐渐衰减，经常更新的参数其学习速率衰减更快。

### RMSProp ？？？

其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。思路类似Momentum SGD，引入一个超参数，在积累梯度平方项进行衰减。

### Adaptive moment estimation (Adam) ？？？

Adam结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。