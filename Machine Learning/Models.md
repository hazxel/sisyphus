# 算法分类

### 有监督学习

使用标记的数据输入到模型中，使模型调整其权重，直到适当地拟合。

##### 弱监督学习

- 不完全监督：训练数据中有些已标记，有些未标记 （也称半监督学习）
- 不确切监督：训练数据只给出了粗粒度标签
- 不精确监督：给出的标签不总是正确的

##### 集成学习

组合多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。常见的模型集成方法：

- Bagging (**B**ootstrap **agg**regat**ing**): 对训练集进行有放回抽样得到子训练集，基于不同子训练集训练基学习器，最后综合所有基学习器的预测值进行预测。（常用投票法）
- Boosting: 每个基模型在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值（常用加权法）
- Stacking: 先用全部数据训练多个不同的模型，再把这些模型的输出作为输入来训练一个模型。

### 无监督学习

使用机器学习算法来分析未标记的数据集并进行聚类，如 K-means clustering。

### 强化学习

强化学习将模型置于一个环境中，模型不断的与环境进行交互，基于环境的反馈而行动试错。强化学习的数据是序列的、交互的、并且还是有反馈的。

强化学习训练过程中，模型“试错”获得的反馈，有时可能需要等到整个训练结束以后才会得到，比如游戏的 Game Over 或 Win。因此我们在训练时候一般会对目标进行拆解，尽量将反馈分解到每一步，引导模型每一步的决策。



# 问题分类

### 回归 regression

在统计学中，回归分析指的是确定两种或以上变量间相互依赖的定量关系的一种统计分析方法。

- 按照**自变量**的多少，可分为一元回归和多元回归
- 按照**因变量**的多少，可分为简单回归和多变量回归（复回归）
- 按照自变量和因变量之间的关系类型，可分为线性回归和非线性回归

### 聚类

### 分类



# 决策树 Decision Tree

决策树是一种监督学习算法，可作为分类算法，也可用于回归模型。决策树中每个非叶节点表示一个判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果。训练时，输入已知分类结果的数据，从决策树根节点出发，尝试进行分类，当一个节点无法给出判断时，将这一节点分成2个。

- ID3：采用自顶向下的贪婪搜索遍历可能的决策树空间，选择信息增益最大的特征进行分裂

- C4.5：克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。
- CART：使用二分法简化决策树的分支规模，提高生成决策树的效率。
- 随机森林 Random Forest：以决策树为基学习器构建 Bagging 集成，且在训练过程中引入随机特征选择，以建立很多不相关的决策树（森林）。每棵决策树都会对输入分别进行判断，然后基于投票法给出分类结果。
- 自适应增强 AdaBoost (Adaptive Boosting)：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。
- GBDT（Gradient Boosting Decision Tree）使用 Boosting 集成模型，被公认泛化能力较强。
- XGBoost: 基于 Boosting 框架的树模型主流集成算法

### 剪枝

为什么要剪枝：过拟合的树在泛化能力的表现非常差。



